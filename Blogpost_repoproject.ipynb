{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction project: Estimating individual treatment effect\n",
    "Julia Wilbers (4470249) J.E.Wilbers@student.tudelft.nl\n",
    "Juan Molano (5239540) J.E.MolanoValencia@student.tudelft.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "The goal of this project was to recreate the neural network, and the results (partly) of the following paper: Estimating individual treatment effect: generalization bounds and algorithms (Uri Shalit et al.). [1] The authors of this paper aim to predict the individual treatment effect (ITE) from observational data. Therefore, the authors proposed a CFR (Counterfactual Regression) framework. In addition, the authors used a Tensorflow framework for the creating of their network. In this project we aim to reconstruct their network in pytorch.\n",
    "\n",
    "It should be noted that the authors of the original paper designed the code in such way that a lot of options are adjustable. For example different types of loss functions, regularisation methods, number of layers etc. In this project we choose to not experiment with all these different settings and mostly use the settings as defined in code (default settings). The only setting from the default configuration that we changed was the choice of the imbalance measure (see section: Correction for distribution difference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Individual treatment effect\n",
    "The individual treatment effect (ITE) can be seen as the effect of a certain treatment for a specific patient. One can imagine that it is very useful to estimate the outcome of a given treatment before the patient receive the treatment.A doctor could then choose the treatment based on the estimated outcome. Observational data e.g. given medication is used order to make such predictions. The ITE can be calculated using the following formula:\n",
    "\n",
    "$ITE (x) = E[Y_1 - Y_0 | x]$\n",
    "\n",
    "Where $Y_1$ is the outcome given the treatment and $Y_0$ the outcome not given the treatment. Then the ITE of a certain treatment is then the expected outcome given the treatment minus the expected outcome not given the treatment when observing data x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IHDP dataset\n",
    "This project makes use of the IHDP (Infant Health and Development Program) dataset, in this dataset consist of data from early born baby's and their mother. It includes data of 747 patients from whom 25 covariates (x) where measured for a total of 100 treatments. [2],[1]\n",
    "\n",
    "![Table 1](images/table_data.png)\n",
    "\n",
    "This table shows the structure of the dataset. Adapted from [3].\n",
    "The value t {1,0} indicates whether a patient is treated (1) or not (0), the y_factual is the outcome and the yc_factual is the outcome if one would have give the opposite treatment. This for example: if a diabetic patient received an insuline in treatment the t value would be 1 for the insulin treatment and the y_factual would be the \"factual\" outcome, the yc_factual outcome would then be the outcome given the opposite treatment, so when no insulin was given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CFR-net\n",
    "For the recreating of the CFR network we firstly stepwise unraveled the Tensorflow code made by the authors. For this purpose we created a flowchart to gain insight in the proposed network:\n",
    "\n",
    "![Flowchart](images/Flowdiagram.png)\n",
    "\n",
    "It can be seen that the network consist of N layers (this is adjustable, but we choose for the default setting of 2 representation and 2 regression layers), the ReLU a non-linear activation, uses multiple dropout layers and at half-way it concatenates with the t-values. Then the user can choose for splitting the data set in two subsets: a subset with all data where t = 1 (treated group), and a subset for all data with t = 0 (not treated). If the user choose to do so then the datasets will be merged again to form 1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "split_output = False\n",
    "class FCNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple fully connected neural network with residual connections in PyTorch.\n",
    "    Layers are defined in __init__ and forward pass implemented in forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FCNet, self).__init__()\n",
    "\n",
    "        p = 0.3\n",
    "\n",
    "        # Creating the linear layers\n",
    "        self.h_in = nn.Linear(25, 100)\n",
    "        self.layer_1 = nn.Linear(100, 100)\n",
    "        self.layer_2 = nn.Linear(100, 100)\n",
    "        self.layer_3 = nn.Linear(101, 100)\n",
    "        self.layer_4 = nn.Linear(100, 100)\n",
    "        self.layer_5 = nn.Linear(100, 100)\n",
    "\n",
    "        # Creating the dropout layers\n",
    "        self.do1 = torch.nn.Dropout(p=p)\n",
    "        self.do2 = torch.nn.Dropout(p=p)\n",
    "        self.do3 = torch.nn.Dropout(p=p)\n",
    "        self.do4 = torch.nn.Dropout(p=p)\n",
    "        self.do5 = torch.nn.Dropout(p=p)\n",
    "        self.do6 = torch.nn.Dropout(p=p)\n",
    "\n",
    "        # Creating the linear classifier layer\n",
    "        self.fc6 = nn.Linear(100,1)\n",
    "\n",
    "    # Forward pass for the first half of the network\n",
    "    def forward(self, x, t):\n",
    "        h = self.do1(F.relu(self.h_in(x)))\n",
    "        h = self.do2(F.relu(self.layer_1(h)))\n",
    "        h_rep = self.do3(F.relu(self.layer_2(h)))\n",
    "        h = self._build_output_graph( h, t)\n",
    "        self.h_rep = h_rep\n",
    "\n",
    "        return h, h_rep\n",
    "\n",
    "    # Forward pass for the second half of the network\n",
    "    def _build_output (self, h):\n",
    "        h = self.do4(F.relu(self.layer_3(h)))\n",
    "        h = self.do5(F.relu(self.layer_4(h)))\n",
    "        h = self.do6(F.relu(self.layer_5(h)))\n",
    "        h = self.fc6(h)\n",
    "        return h\n",
    "\n",
    "    # Function for splitting data OR concatenate with t\n",
    "    def _build_output_graph(self, h, t):\n",
    "        t = torch.round(t)\n",
    "        if split_output :\n",
    "            i0 = torch.where(t < 1)\n",
    "            i1 = torch.where(t > 0)\n",
    "\n",
    "            temp=torch.index_select(h, 1, i0[1])\n",
    "            rep0 = torch.cat((torch.index_select(h, 1, i0[1]),i0[0]),2)\n",
    "            rep1 = torch.cat((torch.index_select(h, 1, i1[1]),i1[0]),2)\n",
    "\n",
    "            y0 = self._build_output(rep0)\n",
    "            y1 = self._build_output(rep1)\n",
    "\n",
    "            y = dynamic_stitch([i0, i1], [y0, y1])\n",
    "        else:\n",
    "            h = torch.cat((h,t),1)\n",
    "            y = self._build_output(h)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "The training loop of the CFR network makes use of a ADAM optimizer and a MSE-loss criterion. In addition to the loss function there was also corrected for the distribution difference these corrections were added to the loss function. How this was done will be further described in the next section. Further we did not use any regularisation parameters, however the code posted by the authors gives different options for regularisation, but in the default settings it is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Corrections for distribution difference\n",
    "### Sample re-weighting\n",
    "To compensate for the difference in group size (treated / non-treated) a sample re-weighting was introduced. All the samples were re-weighted with the following formula [1]:\n",
    "\n",
    "$wi = \\frac{ti}{2u} + \\frac{1-ti}{2(1-u)}$ for $i = 1 ... n $\n",
    "\n",
    "With t {1,0} and u, the treatment prediction i.e. the chance of being treated based on the patients in this dataset. So for example if a patient has received a certain treatment let's say a patient had a surgery. Then the sample re-weighting would be 2 divided by two times the probability of being treated. If the probability of being treated is really high, close to 1, then a sample will get a relatively low weight. However, if the chance of being treated is very low then the sample would get a higher weight. The same could be observed for t = 0 (not treated). This will somewhat correct for the unequally distributed treatments groups i.e. it will correct for the fact that maybe a lot of patients received treatment A and only a few patients received treatment B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    def sample_reweighting(t,p_t):\n",
    "        w_t = t / (2 * p_t)             #p_t: chance of being treated\n",
    "        w_c = (1 - t) / (2 * 1 - p_t)\n",
    "        sample_weight = w_t + w_c\n",
    "        return sample_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imbalance error\n",
    "In addition to the re-weighting of the samples an imbalance error was introduced in the loss function. The imbalance error adjusts for the bias induced by the treatment group imbalance. There are different methods for the calculation of the imbalance error. For this project the squared linear Maximum Mean discrepancy (MMD) and the Wasserstein methods were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Squared linear MMD imbalance error\n",
    "def mmd2_lin(X,t,p):\n",
    "    ''' Linear MMD '''\n",
    "\n",
    "    it = torch.where(t>0)[0] # getting the positions\n",
    "    ic = torch.where(t<1)[0]\n",
    "\n",
    "    Xt = torch.index_select(X, 0, it) # Getting the nx100 for each value\n",
    "    Xc = torch.index_select(X, 0, ic)\n",
    "\n",
    "    mean_control = torch.mean(Xc,0) # mean of 1x100\n",
    "    mean_treated = torch.mean(Xt,0)\n",
    "\n",
    "    mmd = torch.sum(torch.square(2.0*p*mean_treated - 2.0*(1.0-p)*mean_control))\n",
    "\n",
    "    return mmd\n",
    "\n",
    "# Wasserstein imbalance error\n",
    "def wasserstein(X,t,p,lam=10,its=10,sq=False,backpropT=False):\n",
    "    \"\"\" Returns the Wasserstein distance between treatment groups \"\"\"\n",
    "\n",
    "    it = torch.where(t > 0)[0]  # getting the positions\n",
    "    ic = torch.where(t < 1)[0]\n",
    "\n",
    "    Xt = torch.index_select(X, 0, it)  # Getting the nx100 for each value\n",
    "    Xc = torch.index_select(X, 0, ic)\n",
    "\n",
    "    nc = Xc.shape[0]\n",
    "    nt = Xt.shape[0]\n",
    "\n",
    "    ''' Compute distance matrix'''\n",
    "    if sq:\n",
    "        M = pdist2sq(Xt,Xc)\n",
    "    else:\n",
    "        M = safe_sqrt(pdist2sq(Xt,Xc))\n",
    "\n",
    "    ''' Estimate lambda and delta '''\n",
    "    M_mean = torch.mean(M)\n",
    "    M_drop = torch.nn.Dropout(10/(nc*nt))(M)\n",
    "    delta = torch.max(M)\n",
    "    eff_lam = lam/M_mean\n",
    "\n",
    "    ''' Compute new distance matrix '''\n",
    "    Mt = M\n",
    "    row = delta*torch.ones(M.shape[1])\n",
    "    col = torch.cat((delta*torch.ones(M.shape[0]),torch.zeros((1))),0)\n",
    "    Mt = torch.cat((M, torch.unsqueeze(row, 0)), 0)\n",
    "    Mt = torch.cat((Mt, torch.unsqueeze(col, 1)), 1)\n",
    "\n",
    "    ''' Compute marginal vectors '''\n",
    "    a = torch.cat((p * torch.ones((torch.where(t > 0)[0].shape[0],1)) / nt, (1 - p) * torch.ones((1,1))), 0)\n",
    "    b = torch.cat(((1-p) * torch.ones((torch.where(t < 1)[0].shape[0],1)) / nc, p * torch.ones((1,1))), 0)\n",
    "\n",
    "    ''' Compute kernel matrix'''\n",
    "    Mlam = eff_lam*Mt\n",
    "    K = torch.exp(-Mlam) + 1e-6 # added constant to avoid nan\n",
    "    U = K*Mt\n",
    "    ainvK = K/a\n",
    "\n",
    "    u = a\n",
    "    for i in range(0,its):\n",
    "        u = 1.0/(torch.matmul(ainvK,( b / torch.transpose(torch.matmul(torch.transpose(u,0,1),K),0,1))))\n",
    "    v = b/(torch.transpose(torch.matmul(torch.transpose(u,0,1),K),0,1))\n",
    "\n",
    "    T = u*(torch.transpose(v,0,1)*K)\n",
    "\n",
    "    E = T*Mt\n",
    "    D = 2*torch.sum(E)\n",
    "\n",
    "    return D, Mlam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The actual computations of these imbalance errors goes a bit beyond the scope of this blog post. However, it is good to know that there was corrected for the distribution imbalance in two different ways. This will also result in two different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Outcome measures\n",
    "### PEHE\n",
    "One of the main outcome measures of the project is the expected precision in Estimation of Heterogeneous Effect (PEHE). Which can be calculated using the following formula:\n",
    "\n",
    "$ \\epsilon PEHE = \\int_x (\\hat{\\tau_f}(x)-\\tau_f(x))^2 p(x) dx $\n",
    "\n",
    "With:\n",
    "\n",
    "$\\hat{\\tau_f}(x) = f(x,1) - f(x,0)$\n",
    "\n",
    "Where $p(x)$ is the distribution of the data x, $f(x,1)$ is the hypothesis for a treatment given that the patient with data x is treated. And $f(x,0)$ is the hypothesis for a treatment given that the patient with data x is not treated. $\\tau_f(x)$ is the ITE for this given treatment. Therefore, it can be concluded that the PEHE is a measure for the squared difference between hypothetical outcome for a treatment (calculated by the neural network) and the given outcome for the treatment. So it gives insight in how well the network can make predictions. For the actual outcome of this project is the square root of the PEHE used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ATE\n",
    "The second main outcome is the absolute error of the average treatment effect (ATE). Following the paper this can be calculated using the following formula:\n",
    "\n",
    "$\\epsilon ATE = |\\frac{1}{n}\\sum\\limits_{i=1}^{n} (f(x_i,1)-f(x_i,0)) - \\frac{1}{n}\\sum\\limits_{i=1}^{n} (m_1(xi) - m_0(xi))|$\n",
    "\n",
    "Rewriting knowing that $m_1(xi) - m_0(xi)$ is another description for the ITE gives:\n",
    "\n",
    "$\\epsilon ATE = |\\frac{1}{n}\\sum\\limits_{i=1}^{n} \\hat{\\tau}_i - \\frac{1}{n}\\sum\\limits_{i=1}^{n} \\tau_i|$\n",
    "\n",
    "The following code is just a short representation of the lines used for calculating the PEHE and the ATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pehe_ate(mu1, mu0, ycf_p,yf_p):\n",
    "    # Calculation of PEHE\n",
    "    eff = mu1-mu0 # ITE\n",
    "    eff_pred = ycf_p - yf_p\n",
    "    pehe = np.sqrt(np.mean(np.square(eff_pred-eff)))\n",
    "\n",
    "    # Calulation of ATE\n",
    "    ate_pred = np.mean(eff_pred)\n",
    "    bias_ate = ate_pred-np.mean(eff)\n",
    "\n",
    "    return pehe, bias_ate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results\n",
    "After training our network we calculated the outcome measures (PEHE and ATE) for two different imbalance functions. By doing this we recreated the results of the orginal paper. The results are summarized in the following table:\n",
    "\n",
    "<div>\n",
    "    \n",
    "<img src=\"images/results_table.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "In addition to the outcome measures we also kept track of the losses during each iteration. These results can be summarized in a graph plotting the loss during training and the corresponding validation loss for every 100 iterations. We computed graphs for both the imbalance error functions.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/output_graph_wass.png\" width=\"500\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"images/output_graph_mmd.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "Our presented results are in line with the results of the original paper. Keeping in mind that we were not aware of all the exact settings the authors used for the construction of their results. Additionally, about the imbalance error, it is hard to say which function performs the best because the results fluctuate very much. In the repository of our code the file ''main_cfrnet.py'' can be run to replicate our results.\n",
    "\n",
    "We tried to reproduce the network based on the paper, however this had many complications given the amount of 'mathiness' and unclear explanation of the CFR network. To be able to reproduce the network we needed to carefully analyse the published code. This code had many complications with its outdated TensorFlow libraries and its vast amount parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Recommendations\n",
    "The main reason of the difference in results between our reproduction and the original paper is probably because we did not know the network settings used for the creation of the results. These settings are for example: learning rate, dropout probability, regularisation etc. It would have been favorable to investigate in all the different options and selecting the best settings for the network. \n",
    "In addition, we were not able to split the data set in the network. It could be possible that splitting the dataset has an impact (positive or negative) on the results.\n",
    "To try to test many other architectures of the CFR network, a variant of the project, in which the number of layers can be dynamically allocated, was implemented in the branch named Dynamic_network of our repository. This implementation (shown below) had no remarkable (it even went worse) effects in some of the tests performed when changing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple fully connected neural network with residual connections in PyTorch.\n",
    "    Layers are defined in __init__ and forward pass implemented in forward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, n_in, dim_out, n_out, dropout_in, dropout_out):\n",
    "        super(FCNet, self).__init__()\n",
    "\n",
    "        self.in_layers=[]\n",
    "        self.in_drop_layers = []\n",
    "        self.out_layers = []\n",
    "        self.out_drop_layers = []\n",
    "\n",
    "        # Input layer\n",
    "        self.h_in = nn.Linear(25, dim_in)\n",
    "        self.do1 = torch.nn.Dropout(p=dropout_in)\n",
    "        \n",
    "        # Input group of layers\n",
    "        for i in range(n_in):\n",
    "            self.in_layers.append(nn.Linear(dim_in, dim_in))\n",
    "            self.in_drop_layers.append(torch.nn.Dropout(p=dropout_in))\n",
    "\n",
    "        # Add representation\n",
    "        if not split_output:\n",
    "            self.out_layers.append(nn.Linear(dim_out+1, dim_out))\n",
    "        else:\n",
    "            self.out_layers.append(nn.Linear(dim_out, dim_out))\n",
    "        self.out_drop_layers.append(torch.nn.Dropout(p=dropout_out))\n",
    "\n",
    "         # Output group of layers\n",
    "        for i in range(n_out):\n",
    "            self.out_layers.append(nn.Linear(dim_out, dim_out))\n",
    "            self.out_drop_layers.append(torch.nn.Dropout(p=dropout_out))\n",
    "\n",
    "        # Linear classifier layer\n",
    "        self.h_out = nn.Linear(dim_out, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "[1] Shalit, U. (2017). Estimating individual treatment effect: generalization bounds and algorithms.\n",
    "\n",
    "[2] Information about the IHDP dataset can be found on: https://www.icpsr.umich.edu/web/HMCA/studies/9795\n",
    "\n",
    "[3] Amit Sharma, Emre Kiciman, 2020, accessed on 23-03-2021, https://microsoft.github.io/dowhy/example_notebooks/dowhy_ihdp_data_example.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}